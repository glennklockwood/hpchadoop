#!/bin/bash
################################################################################
#
#  Job script to launch the Hadoop streaming wordcount example on SDSC Gordon
#
#  Glenn K. Lockwood, San Diego Supercomputer Center                 July 2013
#
################################################################################
#PBS -N vcfparse
#PBS -l nodes=4:ppn=1:native
#PBS -l walltime=00:15:00
#PBS -q normal
#PBS -j oe
#PBS -o hadoopcluster.log
#PBS -V

cd $PBS_O_WORKDIR
 
export MY_HADOOP_HOME="/opt/hadoop/contrib/myHadoop"

export HADOOP_CONF_DIR=$PBS_O_WORKDIR/$PBS_JOBID

### Gordon: need to build setenv.sh from hardcoded values in configure.sh
TMPFILE=$(mktemp)
grep '^export' $MY_HADOOP_HOME/bin/configure.sh > $TMPFILE
source $TMPFILE && rm $TMPFILE

### Gordon: Enable Hadoop over Infiniband (add ".ibnet0" suffix to hostnames)
export PBS_NODEFILEZ=$(mktemp)
sed -e 's/$/.ibnet0/g' $PBS_NODEFILE > $PBS_NODEFILEZ

$MY_HADOOP_HOME/bin/configure.sh -n 4 -c $HADOOP_CONF_DIR || exit 1

### Gordon: myHadoop template environment has dummy vars that must be replaced
sed -i 's:^export HADOOP_PID_DIR=.*:export HADOOP_PID_DIR=/scratch/'$USER'/'$PBS_JOBID':' $HADOOP_CONF_DIR/hadoop-env.sh
sed -i 's:^export TMPDIR=.*:export TMPDIR=/scratch/'$USER'/'$PBS_JOBID':' $HADOOP_CONF_DIR/hadoop-env.sh
 
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR namenode -format
 
$HADOOP_HOME/bin/start-all.sh

################################################################################
# Load the python2.7 module, then activate our personal virtualenv
module load python
source ~/mypython/bin/activate

# Preprocess our VCF file and extract the header
python $PWD/parsevcf.py -b ./data.vcf > $PWD/header.txt

# Copy input VCF into HDFS
$HADOOP_HOME/bin/hadoop dfs -mkdir vcfparse
$HADOOP_HOME/bin/hadoop dfs -copyFromLocal ./data.vcf vcfparse/

# Launch the Hadoop streaming job, being sure to share our python2.7 library 
# location with Hadoop and use absolute paths to everything
$HADOOP_HOME/bin/hadoop jar /opt/hadoop/contrib/streaming/hadoop-streaming-1.1.1.jar \
    -mapper "$(which python) $PWD/parsevcf.py -m $PWD/header.txt,0.30" \
    -reducer "$(which python) $PWD/parsevcf.py -r" \
    -input vcfparse/data.vcf \
    -output vcfparse/output \
    -cmdenv LD_LIBRARY_PATH=$LD_LIBRARY_PATH

# Copy output data back out of HDFS
$HADOOP_HOME/bin/hadoop dfs -copyToLocal vcfparse/output $PWD/output

# Print information on how many mappers/reducers were used
echo "*** Printing job status"
jobid=$($HADOOP_HOME/bin/hadoop job -list all | tail -n1 | awk '{print $1}')
$HADOOP_HOME/bin/hadoop job -status $jobid
################################################################################

$HADOOP_HOME/bin/stop-all.sh
cp -Lr $HADOOP_LOG_DIR $PBS_O_WORKDIR/hadoop-logs.$PBS_JOBID
$MY_HADOOP_HOME/bin/pbs-cleanup.sh -n 4
